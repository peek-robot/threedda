{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62968876-2941-4063-8a27-bfa217abd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is modified from https://github.com/haotian-liu/LLaVA/\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from io import BytesIO\n",
    "import os, os.path as osp\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        print(\"downloading image from url\", args.video_file)\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "def center_crop_and_resize(image: Image.Image, crop_size: int, resize_size: int) -> Image.Image:\n",
    "    \"\"\"Center crops an image to `crop_size` and resizes it to `resize_size`.\"\"\"\n",
    "    width, height = image.size\n",
    "    left = (width - crop_size) // 2\n",
    "    top = (height - crop_size) // 2\n",
    "    right = left + crop_size\n",
    "    bottom = top + crop_size\n",
    "    cropped = image.crop((left, top, right, bottom))\n",
    "    return cropped.resize((resize_size, resize_size), Image.LANCZOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3562a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def add_path_2d_to_img(\n",
    "    image, points, line_size=1, circle_size=0, plot_lines=True, color=\"red\"\n",
    "):\n",
    "    img_out = image.copy()\n",
    "\n",
    "    if np.all(points <= 1):\n",
    "        points = points * image.shape[1]\n",
    "\n",
    "    points = points.astype(int)\n",
    "    path_len = len(points)\n",
    "\n",
    "    # Generate gradient from dark red to bright red\n",
    "    if color == \"red\":\n",
    "        color_choice = np.linspace(25, 255, path_len).astype(int)\n",
    "        colors = [tuple(int(r) for r in (r_val, 0, 0)) for r_val in color_choice]\n",
    "    # Generate gradient from dark blue to bright blue\n",
    "    elif color == \"blue\":\n",
    "        color_choice = np.linspace(25, 255, path_len).astype(int)\n",
    "        colors = [tuple(int(r) for r in (0, 0, r_val)) for r_val in color_choice]\n",
    "\n",
    "    for i in range(path_len - 1):\n",
    "        color = colors[i]\n",
    "        if plot_lines:\n",
    "            cv2.line(img_out, tuple(points[i]), tuple(points[i + 1]), color, line_size)\n",
    "        if circle_size > 0:\n",
    "            cv2.circle(\n",
    "                img_out,\n",
    "                tuple(points[i]),\n",
    "                max(1, circle_size),\n",
    "                color,\n",
    "                -1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    # Draw last point\n",
    "    if circle_size > 0:\n",
    "        cv2.circle(\n",
    "            img_out,\n",
    "            tuple(points[-1]),\n",
    "            max(1, circle_size),\n",
    "            colors[-1],\n",
    "            -1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc41ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft example\n",
    "model_name = \"vila_3b_no_hamster\"\n",
    "model_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/checkpoints/finetuned/vila/\"\n",
    "base_name = None\n",
    "base_path = None\n",
    "\n",
    "# LoRA example -> set base_name (and path)\n",
    "model_name = \"vila_3b_sft_all_lora_amazon\"\n",
    "model_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/checkpoints/finetuned/vila/\"\n",
    "base_name = \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "base_path = None\n",
    "\n",
    "############################################\n",
    "\n",
    "model_name = \"vila_3b_all_path_mask\"\n",
    "model_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/checkpoints/finetuned/nvila/\"\n",
    "base_name = None # \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "base_path = None\n",
    "prompt_type = \"path_mask\" # \"mask\", \"path_mask\"\n",
    "\n",
    "model_name = \"nvila_lite_2b_oxe_robopoint\"\n",
    "model_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/checkpoints/finetuned/nvila/\"\n",
    "base_name = None # \"Efficient-Large-Model/VILA1.5-3b\"\n",
    "base_path = None\n",
    "prompt_type = \"path\" # \"mask\", \"path_mask\"\n",
    "\n",
    "args_dict = {\n",
    "    \n",
    "    # replace later\n",
    "    # \"query\": None,\n",
    "    # \"image_file\": None,\n",
    "\n",
    "    # \"video_file\": None,\n",
    "    \"model_path\": model_name if model_path is None else os.path.join(model_path, model_name),\n",
    "    \"conv_mode\": \"vicuna_v1\", # \"llava_v0\", # \"vicuna_v1\",\n",
    "    \n",
    "    \"model_base\": base_name if base_path is None else os.path.join(base_path, base_name),\n",
    "    # \"num_video_frames\": 6,\n",
    "    # \"sep\": \",\",\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 1024,\n",
    "}\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fa712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fs12/portfolios/nvr/users/mmemmel/miniforge3/envs/nvila/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-19 18:54:25,550] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard /lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/checkpoints/finetuned/nvila/nvila_lite_2b_oxe_robopoint None\n"
     ]
    }
   ],
   "source": [
    "version = \"nvila\" if \"nvila\" in model_name else \"vila\"\n",
    "# load_model(version, args)\n",
    "if version == \"vila\":\n",
    "    from llava.constants import IMAGE_TOKEN_INDEX\n",
    "    from llava.model.builder import load_pretrained_model\n",
    "elif version == \"nvila\":\n",
    "    import llava\n",
    "\n",
    "from llava.mm_utils import (get_model_name_from_path, process_images, tokenizer_image_token)\n",
    "from llava.conversation import conv_templates\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "global tokenizer, model, image_processor, context_len\n",
    "\n",
    "# standard model\n",
    "if args.model_base is None:\n",
    "    disable_torch_init()\n",
    "    model_name = get_model_name_from_path(args.model_path)\n",
    "    if version == \"nvila\":\n",
    "        model = llava.load(args.model_path)\n",
    "    elif version == \"vila\":\n",
    "        tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, model_name, args.model_base)\n",
    "    print(\"standard\", args.model_path, args.model_base)\n",
    "\n",
    "# LoRA\n",
    "else:\n",
    "    disable_torch_init()\n",
    "    \n",
    "    from llava.model.builder import load_pretrained_model\n",
    "    from peft import PeftModel\n",
    "    tokenizer, base_model, image_processor, context_len = load_pretrained_model(\n",
    "        args.model_base, get_model_name_from_path(args.model_base), model_base=None\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, args.model_path)\n",
    "\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"LoRA\", args.model_path, args.model_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80606346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nvila(message, args):\n",
    "    outputs = model.generate_content(message)\n",
    "    return outputs.strip()\n",
    "\n",
    "def inference_vila(message, args):\n",
    "    quest = message[0]\n",
    "    images = message[1:]\n",
    "\n",
    "    # conversation template\n",
    "    conv = conv_templates[args.conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], quest)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    # preprocessing\n",
    "    images_tensor = process_images(images, image_processor, model.config).to(model.device, dtype=torch.float16)\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "\n",
    "    # inference\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=[\n",
    "                images_tensor,\n",
    "            ],\n",
    "            do_sample=True if args.temperature > 0 else False,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            num_beams=args.num_beams,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # postprocess\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return outputs.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad64f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-19 18:55:08.769\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>put the carrot in the sink</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<ans>[(0.59, 0.2), (0.52, 0.2), (0.57, 0.25), (0.57, 0.18), (0.5, 0.21), (0.54, 0.32), (0.57, 0.25)]</ans>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vila_utils.utils.prompts import get_prompt\n",
    "\n",
    "quest = \"close the drawer\"\n",
    "image = load_image(\"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/example_imgs/drawer_scene.png\")\n",
    "\n",
    "quest = \"put the carrot in the sink\"\n",
    "image = load_image(\"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/example_imgs/widowx.png\")\n",
    "\n",
    "\n",
    "image = center_crop_and_resize(image, min(image.size), 384)\n",
    "message = [get_prompt(quest, prompt_type), image]\n",
    "if version == \"vila\":\n",
    "    outputs = inference_vila(message, args)\n",
    "elif version == \"nvila\":\n",
    "    outputs = inference_nvila(message, args)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6842a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vila_utils.utils.encode import scale_path\n",
    "from vila_utils.utils.decode import get_path_from_answer, add_mask_2d_to_img\n",
    "\n",
    "def add_answer_to_img(img, answer, prompt_type, color=\"red\"):\n",
    "    \n",
    "    out = get_path_from_answer(answer, prompt_type)\n",
    "\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    # scale path to image size\n",
    "    scaled_mask = None\n",
    "    if \"mask\" in prompt_type:\n",
    "        min_in, max_in = np.zeros(2), np.array([w,h])\n",
    "        min_out, max_out = np.zeros(2), np.ones(2)\n",
    "        mask = out[1] if len(out) == 2 else out\n",
    "        scaled_mask = scale_path(mask, min_in=min_out, max_in=max_out, min_out=min_in, max_out=max_in)\n",
    "\n",
    "    path = None\n",
    "    scaled_path = None\n",
    "    if \"path\" in prompt_type:\n",
    "        min_in, max_in = np.zeros(2), np.array([w,h])\n",
    "        min_out, max_out = np.zeros(2), np.ones(2)\n",
    "        path = out[0] if len(out) == 2 else out\n",
    "        scaled_path = scale_path(path, min_in=min_out, max_in=max_out, min_out=min_in, max_out=max_in)\n",
    "\n",
    "    if \"mask\" in prompt_type and scaled_mask is not None:\n",
    "        img = add_mask_2d_to_img(img, scaled_mask, mask_pixels=int(h*0.15))\n",
    "\n",
    "    if \"path\" in prompt_type and scaled_path is not None:\n",
    "        img = add_path_2d_to_img(img, scaled_path, line_size=3, circle_size=0, plot_lines=True, color=color)\n",
    "\n",
    "    return img, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b02b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[32m2025-05-19 18:55:23.940\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>put small spoon from basket to tray</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:11,  1.23s/it]\u001b[32m2025-05-19 18:55:25.168\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>put the blue cube on the right side of the table on top of the rectangular block</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 20%|██        | 2/10 [00:03<00:13,  1.67s/it]\u001b[32m2025-05-19 18:55:27.144\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>put the red object into the pot</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 30%|███       | 3/10 [00:04<00:11,  1.69s/it]\u001b[32m2025-05-19 18:55:28.854\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>put the rectangle block on top of the red arch and yellow block</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 40%|████      | 4/10 [00:06<00:10,  1.69s/it]\u001b[32m2025-05-19 18:55:30.541\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>take broccoli out of pan</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 50%|█████     | 5/10 [00:08<00:08,  1.70s/it]\u001b[32m2025-05-19 18:55:32.273\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>Picked the piece of chocolate and put it into the drawer</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 60%|██████    | 6/10 [00:10<00:07,  1.80s/it]\u001b[32m2025-05-19 18:55:34.254\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>Move the blue fork to the lower right burner</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 70%|███████   | 7/10 [00:12<00:05,  1.78s/it]\u001b[32m2025-05-19 18:55:35.992\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>open the drawer</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.61s/it]\u001b[32m2025-05-19 18:55:37.231\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>Move the can in the middle bottom of the table</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.71s/it]\u001b[32m2025-05-19 18:55:39.172\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mllava.utils.media\u001b[0m:\u001b[36mextract_media\u001b[0m:\u001b[36m87\u001b[0m - \u001b[33m\u001b[1mMedia token '<image>' found in text: '<image>\n",
      "In the image, please execute the command described in <quest>bmbfbbfgjjg</quest>.\n",
      "Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.\n",
      "Format your answer as a list of tuples enclosed by <ans> and </ans> tags. For example:\n",
      "<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), (0.74, 0.21), ...]</ans>\n",
      "The tuple denotes point x and y location of the end effector of the gripper in the image.\n",
      "The coordinates should be integers ranging between 0.0 and 1.0, indicating the relative locations of the points in the image.\n",
      "'. Removed.\u001b[0m\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "out_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/results\"\n",
    "save_path = os.path.join(out_path, model_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "json_path = \"/lustre/fs12/portfolios/nvr/users/mmemmel/projects/vila/data/oxe_processed_path_subtraj/bridge_v2_primary_path/train_bridge_v2_primary_path_conv.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[:10]\n",
    "\n",
    "logs = {\n",
    "    \"img_path\": [],\n",
    "    \"path_pred\": [],\n",
    "    \"path_gt\": [],\n",
    "    \"dtw_euclidian\": [],\n",
    "    \"first_l2\": [],\n",
    "    \"last_l2\": [],\n",
    "}\n",
    "for qa in tqdm(data):\n",
    "\n",
    "    img_path = qa[\"image\"]\n",
    "    answer_gt = qa[\"conversations\"][1][\"value\"]\n",
    "    prompt = qa[\"conversations\"][0][\"value\"]\n",
    "    image = load_image(img_path)\n",
    "\n",
    "    image = center_crop_and_resize(image, min(image.size), 384)\n",
    "    message = [prompt, image]\n",
    "    if version == \"vila\":\n",
    "        answer_pred = inference_vila(message, args)\n",
    "    elif version == \"nvila\":\n",
    "        answer_pred = inference_nvila(message, args)\n",
    "\n",
    "    image = np.array(image)\n",
    "    image, path_pred = add_answer_to_img(image, answer_pred, \"path\", color=\"red\")\n",
    "    image, path_gt = add_answer_to_img(image, answer_gt, \"path\", color=\"blue\")\n",
    "\n",
    "    # compute metrics\n",
    "    dtw_euclidian, _ = fastdtw(path_gt, path_pred, dist=euclidean)\n",
    "    first_l2 = np.linalg.norm(path_gt[0] - path_pred[0])\n",
    "    last_l2 = np.linalg.norm(path_gt[-1] - path_pred[-1])\n",
    "\n",
    "    # add metrics to image\n",
    "    text = f\"DTW {dtw_euclidian:.2f} first {first_l2:.2f} last {last_l2:.2f}\"\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.75, 2)\n",
    "    cv2.rectangle(image, (8, image.shape[0]-40), (8 + text_width + 4, image.shape[0]-10), (255,255,255), -1)\n",
    "    cv2.putText(image, text, (10, image.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 2)\n",
    "\n",
    "    # save image\n",
    "    img_path = os.path.join(save_path, f\"{img_path.split('/')[-1]}.png\")\n",
    "    Image.fromarray(image).save(img_path)\n",
    "\n",
    "    logs[\"img_path\"].append(img_path)\n",
    "    logs[\"path_pred\"].append(path_pred)\n",
    "    logs[\"path_gt\"].append(path_gt)\n",
    "    logs[\"dtw_euclidian\"].append(dtw_euclidian)\n",
    "    logs[\"first_l2\"].append(first_l2)\n",
    "    logs[\"last_l2\"].append(last_l2)\n",
    "\n",
    "# dump logs to json\n",
    "with open(os.path.join(save_path, \"results.json\"), \"w\") as f:\n",
    "    results = []\n",
    "    for i in range(len(logs[\"img_path\"])):\n",
    "        results.append({\n",
    "            \"img_path\": logs[\"img_path\"][i],\n",
    "            \"path_pred\": logs[\"path_pred\"][i].tolist(),\n",
    "            \"path_gt\": logs[\"path_gt\"][i].tolist(),\n",
    "            \"dtw_euclidian\": logs[\"dtw_euclidian\"][i],\n",
    "            \"first_l2\": logs[\"first_l2\"][i],\n",
    "            \"last_l2\": logs[\"last_l2\"][i],\n",
    "        })\n",
    "    json.dump(results, f)\n",
    "\n",
    "# dump summary to json\n",
    "summary = {\n",
    "    \"dtw_euclidian\": np.mean(logs[\"dtw_euclidian\"]),\n",
    "    \"first_l2\": np.mean(logs[\"first_l2\"]),\n",
    "    \"last_l2\": np.mean(logs[\"last_l2\"]),\n",
    "}\n",
    "with open(os.path.join(save_path, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvila",
   "language": "python",
   "name": "nvila"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
